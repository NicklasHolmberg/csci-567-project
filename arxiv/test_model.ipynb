{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_model(\n",
    "    model, \n",
    "    test_loader, \n",
    "    load_path=\"best_model.pth\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the best saved model on test data and display results.\n",
    "    \"\"\"\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"\\nEvaluating the best model on test data...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Metrics calculation\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Collect predictions and labels for confusion matrix\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # GPU memory management\n",
    "            del images, labels, outputs\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions, labels=np.arange(num_classes))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Classification Report\n",
    "    if classes:\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_predictions, target_names=classes))\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes if classes else np.arange(num_classes))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix: Test Data\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, test_cm = evaluate_best_model(\n",
    "    model=model, \n",
    "    test_loader=test_loader, \n",
    "    load_path=\"best_model.pth\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS567",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
